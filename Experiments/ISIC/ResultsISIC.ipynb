{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01524cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re \n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import text \n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision.io import read_image\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.nn.functional import mse_loss as mse\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data as tdata\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle \n",
    "from datetime import datetime\n",
    "import torchvision.models as models\n",
    "from kornia.losses import SSIMLoss, PSNRLoss\n",
    "from kornia.metrics import psnr, ssim\n",
    "import torch.utils.tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "torch.set_printoptions(linewidth=120)\n",
    "from torchsummary import summary\n",
    "import json\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "torch.manual_seed(1996)\n",
    "torch.cuda.manual_seed(1996)\n",
    "np.random.seed(1996)\n",
    "random.seed(1996)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1bb7f",
   "metadata": {},
   "source": [
    "# Load the required models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class costumeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    data_path: path of all the images as a list, each element in the list is path of an image\n",
    "    label_map: dataframe of corresponding labels that is loaded: maps label to image-id\n",
    "    class_map: a dictionary of classes: maps labels to encoded labels\n",
    "    patient_map: a dataframe that maps patient_id to frequency of each label among the images of that patient\n",
    "    label_col: the column in label file where the labels are saved: here is diagnosis\n",
    "    transform: the type of transform. As default it is none, meaning that the images will only be transformed to tensors\n",
    "    it can also have augmentations\n",
    "    noisy_transform: the type of noise implemented in the image. default: none, no noisy data\n",
    "    val_split: val_train ratio. default: None, no validation split.\n",
    "    \n",
    "    Return Dataset class representing our data set\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 data_dir, \n",
    "                 label_map, \n",
    "                 class_map, \n",
    "                 #patient_map,\n",
    "                 label_col, \n",
    "                 augment = None, \n",
    "                 noisy_transform= None,\n",
    "                 image_size = (240,240)\n",
    "                ):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.label_map = label_map \n",
    "        self.augment = augment\n",
    "        self.noisy_transform = noisy_transform\n",
    "        self.class_map = class_map\n",
    "        self.label_col = label_col\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self): \n",
    "        '''returns size of the dataset'''\n",
    "        return len(self.label_map)\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        'Generates one sample of data'  \n",
    "        img_path = self.data_dir+\"/\"+self.label_map.loc[index, self.label_col]\n",
    "        img = Image.open(img_path)\n",
    "        label = self.class_map[self.label_map.loc[index, \"label\"]]\n",
    "  \n",
    "        if self.transform: \n",
    "            image = self.transform(img)       \n",
    "        if self.augment: \n",
    "            augment_image = self.augment(image)\n",
    "            if self.noisy_transform: #here it is a mix of all five transforms\n",
    "                noisy_image = self.noisy_transform(augment_image)\n",
    "                return augment_image, noisy_image, label\n",
    "\n",
    "        else: \n",
    "            if self.noisy_transform: #here it is a mix of all five transforms\n",
    "                noisy_image = self.noisy_transform(image)\n",
    "                return image, noisy_image, label\n",
    "            \n",
    "                \n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, stride=2, padding=1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2, padding=1) \n",
    "    \n",
    "        # Decoder\n",
    "        self.deconv4 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128*2, 64, 4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64*2, 32, 4, stride=2, padding=1)    \n",
    "        self.deconv1 = nn.ConvTranspose2d(32*2, 3, 4, stride=2, padding=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        self.act_fn = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.out_fn = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):      \n",
    "        #AE architecture\n",
    "        # Encoder\n",
    "        z1 = self.conv1(x)       \n",
    "        z1 = self.act_fn(z1)\n",
    "        z1 = self.dropout(z1)  \n",
    "        \n",
    "        z2 = self.conv2(z1)        \n",
    "        z2 = self.act_fn(z2)\n",
    "        z2 = self.dropout(z2)\n",
    "        \n",
    "        z3 = self.conv3(z2)      \n",
    "        z3 = self.act_fn(z3)\n",
    "        z3 = self.dropout(z3)\n",
    "        \n",
    "        z4 = self.conv4(z3)      \n",
    "        z = self.act_fn(z4) \n",
    "        # Decoder\n",
    "        x_hat = self.deconv4(z)\n",
    "        x_hat = self.act_fn(x_hat)\n",
    "        x_hat = torch.cat((x_hat,z3),1)  \n",
    "        x_hat = self.deconv3(x_hat)\n",
    "        x_hat = self.act_fn(x_hat)\n",
    "        x_hat = torch.cat((x_hat,z2),1)   \n",
    "        x_hat = self.deconv2(x_hat)\n",
    "        x_hat = self.act_fn(x_hat)\n",
    "        x_hat = torch.cat((x_hat,z1),1)     \n",
    "        x_hat = self.deconv1(x_hat)\n",
    "        x_hat = self.out_fn(x_hat)\n",
    "        return {'z': z, 'x_hat': x_hat}\n",
    "    \n",
    "\"_______________set experiment__________________\"\n",
    "def set_experiment(model, params):\n",
    "    # Transform\n",
    "    composed = {\n",
    "        'augment': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            #transforms.Resize(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomAffine(degrees=(10), scale=(0.8,1.2)),\n",
    "            transforms.ToTensor()\n",
    "            ]),\n",
    "        \n",
    "        'noisy': transforms.Compose([\n",
    "            transforms.ToPILImage(),            \n",
    "            transforms.RandomApply([transforms.ColorJitter(brightness=[params[\"brightness\"], params[\"brightness\"]])],p=params[\"p\"]),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "    }\n",
    "    data_dir = \"ISIC/data\"\n",
    "    train_path = \"ISIC/labels_train.csv\"\n",
    "    test_path = 'ISIC/labels_test.csv'\n",
    "    valid_path = 'ISIC/labels_valid.csv'\n",
    "    added_noise_appendix_path = \"ISIC/labels_added_noise_appendix.csv\"\n",
    "    added_noise_path = 'ISIC/labels_added_noise.csv'\n",
    "    natural_path = 'ISIC/labels_natural.csv'\n",
    "    \n",
    "    \n",
    "    \n",
    "    class_map = {\"melanoma\":0,\"nevus\":1, \"pigmented benign keratosis\":2, \"basal cell carcinoma\":3}\n",
    "    label_col = \"imageId\"\n",
    "    train_file, valid_file, test_file,added_noise_appendix_file ,added_noise_file ,natural_file  = pd.read_csv(train_path), pd.read_csv(valid_path), pd.read_csv(test_path),pd.read_csv(added_noise_appendix_path, sep=\";\"),pd.read_csv(added_noise_path, sep=\";\"),pd.read_csv(natural_path, sep=\";\")\n",
    "    train_file = train_file.loc[:, ~train_file.columns.str.contains('Unnamed')]\n",
    "    valid_file = valid_file.loc[:, ~valid_file.columns.str.contains('Unnamed')]\n",
    "    test_file = test_file.loc[:, ~test_file.columns.str.contains('Unnamed')]\n",
    "    \n",
    "    added_noise_appendix_file = added_noise_appendix_file.loc[:, ~added_noise_appendix_file.columns.str.contains('Unnamed')]\n",
    "    added_noise_file = added_noise_file.loc[:, ~added_noise_file.columns.str.contains('Unnamed')]\n",
    "    natural_file = natural_file.loc[:, ~natural_file.columns.str.contains('Unnamed')]\n",
    "    \n",
    "    added_noise_appendix_labels = added_noise_appendix_file.label.tolist()\n",
    "    added_noise_labels = added_noise_file.label.tolist()\n",
    "    natural_labels = natural_file.label.tolist()\n",
    "    train_dataset = costumeDataset(data_dir = data_dir, \n",
    "             label_map = train_file, \n",
    "             class_map = class_map, \n",
    "             label_col = label_col, \n",
    "             augment = composed[\"augment\"], \n",
    "             noisy_transform= composed[\"noi\n",
    "             image_size = params['image_size'])\n",
    "    \n",
    "    test_dataset = costumeDataset(data_dir = data_dir, \n",
    "             label_map = test_file, \n",
    "             class_map = class_map, \n",
    "             label_col = label_col, \n",
    "             augment = None, \n",
    "             noisy_transform= composed[\"noisy\"], \n",
    "             image_size = params['image_size'])\n",
    "\n",
    "    valid_dataset = costumeDataset(data_dir = data_dir, \n",
    "             label_map = valid_file, \n",
    "             class_map = class_map, \n",
    "             label_col = label_col, \n",
    "             augment = None, \n",
    "             noisy_transform= composed[\"noisy\"], \n",
    "             image_size = params['image_size'])\n",
    "   \n",
    "    added_noise_appendix_dataset = costumeDataset(data_dir = data_dir, \n",
    "             label_map = added_noise_appendix_file, \n",
    "             class_map = class_map, \n",
    "             label_col = label_col, \n",
    "             augment = None, \n",
    "             noisy_transform= composed[\"noisy\"], \n",
    "             image_size = params['image_size'])\n",
    "    \n",
    "    added_noise_dataset = costumeDataset(data_dir = data_dir, \n",
    "             label_map = added_noise_file, \n",
    "             class_map = class_map, \n",
    "             label_col = label_col, \n",
    "             augment = None, \n",
    "             noisy_transform= composed[\"noisy\"], \n",
    "             image_size = params['image_size'])\n",
    "    \n",
    "    natural_dataset = costumeDataset(data_dir = data_dir, \n",
    "             label_map = natural_file, \n",
    "             class_map = class_map, \n",
    "             label_col = label_col, \n",
    "             augment = None, \n",
    "             noisy_transform= composed[\"noisy\"], \n",
    "             image_size = params['image_size'])\n",
    "\n",
    "    \n",
    "\n",
    "    #set data iterators\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=params['batch_size'], shuffle=False, \n",
    "        num_workers=4, pin_memory=False,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, batch_size=params['batch_size'], shuffle=False, \n",
    "        num_workers=4, pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    test_loader =  DataLoader(\n",
    "        test_dataset, batch_size=params['batch_size'], shuffle=False, \n",
    "        num_workers=4, pin_memory=False,\n",
    "    )\n",
    "    added_noise_appendix_loader = DataLoader(\n",
    "        added_noise_appendix_dataset, batch_size=params['batch_size'], shuffle=False, \n",
    "        num_workers=4, pin_memory=False,\n",
    "    )\n",
    "    added_noise_loader = DataLoader(\n",
    "        added_noise_dataset, batch_size=params['batch_size'], shuffle=True, \n",
    "        num_workers=4, pin_memory=False,\n",
    "    )\n",
    "    natural_loader = DataLoader(\n",
    "        natural_dataset, batch_size=params['batch_size'], shuffle=False, \n",
    "        num_workers=4, pin_memory=False,\n",
    "    )\n",
    "    \n",
    "    # Get experiment modules\n",
    "    experiment_modules ={\n",
    "        'augment' : composed[\"augment\"],\n",
    "        'noisy': composed[\"noisy\"],\n",
    "        'train_loader' : train_loader, \n",
    "        'valid_loader' : valid_loader,\n",
    "        'test_loader': test_loader,\n",
    "        'added_noise_appendix_loader': added_noise_appendix_loader,\n",
    "        'added_noise_loader': added_noise_loader,\n",
    "        'natural_loader':natural_loader,\n",
    "        'added_noise_appendix_labels': added_noise_appendix_labels,\n",
    "        'added_noise_labels': added_noise_labels,\n",
    "        'natural_labels': natural_labels\n",
    "    }\n",
    "    return experiment_modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc31f2",
   "metadata": {},
   "source": [
    "# Loading The Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P=0.1\n",
    "with open('HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8.pickle', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "data_items = x. items()\n",
    "data_list = list(data_items)\n",
    "df1 = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad885532",
   "metadata": {},
   "source": [
    "# Get Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "def get_learningCurve(df, b):\n",
    "    epoch = df.iloc[0,1]\n",
    "    train_loss = df.iloc[1,1] #ssim train\n",
    "    test_loss = df.iloc[8,1] # ssim valid\n",
    "    psnr_train = df.iloc[7,1]\n",
    "    psnr_test = df.iloc[15,1]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2,figsize=(15,10))\n",
    "    axs[0].plot(epoch,train_loss, label='Train')\n",
    "    axs[0].plot(epoch,test_loss, label='Test')\n",
    "    axs[0].set_title('SSIM Loss', fontsize=16, fontweight=\"bold\")\n",
    "    axs[0].set_xlabel('Epoch', fontsize=16, fontweight=\"bold\")\n",
    "    axs[0].set_ylabel('SSIM Loss', fontsize=16, fontweight=\"bold\")\n",
    "    axs[0].legend(loc='best')\n",
    "\n",
    "    axs[1].plot(epoch,psnr_train, label='Train')\n",
    "    axs[1].plot(epoch,psnr_test, label='Test')\n",
    "    axs[1].set_title('PSNR Score', fontsize=16, fontweight=\"bold\")\n",
    "    axs[1].set_xlabel('Epoch', fontsize=16, fontweight=\"bold\")\n",
    "    axs[1].set_ylabel('PSNR Score', fontsize=16, fontweight=\"bold\")\n",
    "    axs[1].legend(loc='best')\n",
    "    fig.suptitle(f'Learning Curve on ISIC Dataset', fontsize=20, fontweight=\"bold\")\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"LearningCurve_BENCHMARK_ISIC_{b}.png\")     \n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "get_learningCurve(df, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935eef93",
   "metadata": {},
   "source": [
    "# Get psnr and ssim of the last epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for different noise levels of benchmark\n",
    "def get_ssim_psnr_noisy_model(model_file,b, params):\n",
    "    model = torch.load(model_file)\n",
    "    model.cuda()\n",
    "    experiment_modules = set_experiment(model, params)\n",
    "    #setup metrics\n",
    "    \n",
    "    \n",
    "    rssimnx = 0.0\n",
    "    rpsnrnx = 0.0\n",
    "    rssimxh = 0.0\n",
    "    rpsnrxh = 0.0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        for x, noisy_x, y in experiment_modules[\"test_loader\"]:\n",
    "            x, noisy_x, y= x.cuda(), noisy_x.cuda(), y.cuda()\n",
    "            \n",
    "            outputs = model(noisy_x) \n",
    "            x_hat = outputs['x_hat']\n",
    "\n",
    "            ssimnoisyxtmp = ssim(x,noisy_x, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimnoisyx = ssimnoisyxtmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            msenoisyxtmp = mse(x,noisy_x, reduction = \"none\") # BxCxHxW\n",
    "            msexhatxtmp = mse(x,x_hat, reduction = \"none\")# BxCxHxW\n",
    "            msenoisyx = msenoisyxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            msexhatx = msexhatxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            psnrnoisyx = 10.0 * torch.log10(1.0 ** 2 / msenoisyx) #B\n",
    "            psnrxxhat = 10.0 * torch.log10(1.0 ** 2 / msexhatx) #B\n",
    "            \n",
    "            \n",
    "            \n",
    "            ssimxxhattmp = ssim(x,x_hat, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimxxhat = ssimxxhattmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            \n",
    "            rssimnx += ssimnoisyx.sum(0)\n",
    "            rssimxh += ssimxxhat.sum(0)\n",
    "            \n",
    "            rpsnrxh += psnrxxhat.sum(0)\n",
    "            rpsnrnx += psnrnoisyx.sum(0)\n",
    "            #visualise\n",
    "            \n",
    "            plt.imshow(np.transpose(x[2].to(\"cpu\"), (1, 2, 0)))\n",
    "            plt.show()\n",
    "            plt.imshow(np.transpose(noisy_x[2].to(\"cpu\"), (1, 2, 0)))\n",
    "            plt.show()\n",
    "            plt.imshow(np.transpose(x_hat[2].to(\"cpu\"), (1, 2, 0)))\n",
    "            plt.show()\n",
    "            print(\"ssim of this image\", ssimnoisyx[2].item(), ssimxxhat[2].item())\n",
    "            print(\"psnr of this image\", psnrnoisyx[2].item(), psnrxxhat[2].item())\n",
    "                 \n",
    "    n = 399\n",
    "    meanssimnx = rssimnx/n\n",
    "    meanssimxh = rssimxh/n\n",
    "    \n",
    "    meanpsnrnx = rpsnrnx/n\n",
    "    meanpsnrxh = rpsnrxh/n\n",
    "    return meanssimnx, meanssimxh, meanpsnrnx, meanpsnrxh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac02abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the psnr ssim table\n",
    "def save_psnr_ssim_table(model_b):\n",
    "    \n",
    "    shared_params = {\n",
    "     'num_epochs': 80,\n",
    "     'batch_size': 15,\n",
    "     'random_seed': 1996,\n",
    "     'optimizer_name': 'adamW',\n",
    "     'learning_rate': 0.001, #Alfia: 0.00001\n",
    "     'image_size': (240,240),\n",
    "     'degrees': (-10, 10),\n",
    "     'translate': (0.0, 0.5),\n",
    "     'scale': (0.5, 0.95),\n",
    "     'dropout_p': 0.5,# dropout probability\n",
    "     'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     'alpha':1, #weight of ssim\n",
    "     'weight_decay':1e-3 # L2 regularization\n",
    "    }\n",
    "    \n",
    "    model_file = f\"../HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}epoch_80\"\n",
    "\n",
    "    dic = {\"noise_level\":[], \"mean_ssim_noisy_x\":[], \"mean_ssim_xhat_x\":[] ,\"mean_psnr_noisy_x\":[], \"mean_psnr_xhat_x\":[]}\n",
    "    b_list = np.arange(round(1.0-model_b,1), round(1.0+model_b+0.2,1), 0.2)\n",
    "    print(\"=======================================\")\n",
    "    print(\"model_file\", model_file, \"model_b\", model_b, \"Noise_Level_Range\", b_list)\n",
    "    for b in b_list:\n",
    "        params = { \n",
    "\n",
    "            \"gamma\": 0.01,\n",
    "            \"p\": 1,\n",
    "            \"brightness\": round(b,1) # is noise level\n",
    "            }\n",
    "        params = {**shared_params, **params} \n",
    "        print(\"noise_level\", round(b,1), params[\"brightness\"])\n",
    "        meanssimnx, meanssimxh, meanpsnrnx, meanpsnrxh = get_ssim_psnr_noisy_model(model_file, round(b,1), params)\n",
    "        dic[\"noise_level\"].append(round(b,1)) \n",
    "        dic[\"mean_ssim_noisy_x\"].append(meanssimnx.item()) \n",
    "        dic[\"mean_ssim_xhat_x\"].append(meanssimxh.item()) \n",
    "        dic[\"mean_psnr_noisy_x\"].append(meanpsnrnx.item()) \n",
    "        dic[\"mean_psnr_xhat_x\"].append(meanpsnrxh.item())\n",
    "        with open(f'exp1_benchmark/MODEL{model_b}_SSIM_PSNR_TABLE_train.pickle','wb') as handle: \n",
    "            pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f78ba9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_b_list = [0.8]\n",
    "for model_b in model_b_list:\n",
    "    dic = save_psnr_ssim_table(round(model_b,1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05618628",
   "metadata": {},
   "source": [
    "# Images and qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c51ffc",
   "metadata": {},
   "source": [
    "## Added Noise report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d289d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualise_images_added_noise_report(model_file, b_list, shared_params):\n",
    "    ssim_psnr_values = {\"ssim_noisy_x\":[],\"ssim_xhat_x\":[],\"psnr_noisy_x\":[],\"psnr_xhat_x\":[]} # b_list x B x 1 each\n",
    "    fig, axs = plt.subplots(len(b_list)*2+1, shared_params[\"batch_size\"],figsize=(15,15), subplot_kw={'xticks': [], 'yticks': []}) #hight same as batch_size, width = GT + len(b_list) *2\n",
    "    # build a rectangle in axes coord\n",
    "    left, width = .25, .5\n",
    "    bottom, height = .25, .5\n",
    "    right = left + width\n",
    "    top = bottom + height\n",
    "    added_noise_labels = [\"Melanoma1\", \"Nevus\", \"BCC1\", \"PBK1\", \"Melanoma2\", \"PBK2\", \"BCC2\"]\n",
    "    \n",
    "    model = torch.load(model_file)\n",
    "    model.cuda()\n",
    "    \n",
    "    for b in range(len(b_list)):\n",
    "        params = { \n",
    "            \"gamma\": 0.01,\n",
    "            \"brightness\": round(b_list[b],1) # is noise level\n",
    "            }\n",
    "        params = {**shared_params, **params}\n",
    "        experiment_modules = set_experiment(model, params)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            x, noisy_x, y = next(iter(experiment_modules[\"added_noise_loader\"]))\n",
    "            x, noisy_x, y = x.cuda(), noisy_x.cuda(), y.cuda()\n",
    "            outputs = model(noisy_x) \n",
    "            #y_hat = outputs['y_hat'].cuda()\n",
    "            x_hat = outputs['x_hat']\n",
    "            # qualitative scores\n",
    "            ssimnoisyxtmp = ssim(x,noisy_x, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimnoisyx = ssimnoisyxtmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            ssimxxhattmp = ssim(x,x_hat, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimxxhat = ssimxxhattmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            \n",
    "            msenoisyxtmp = mse(x,noisy_x, reduction = \"none\") # BxCxHxW\n",
    "            msexhatxtmp = mse(x,x_hat, reduction = \"none\")# BxCxHxW\n",
    "            msenoisyx = msenoisyxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            msexhatx = msexhatxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            psnrnoisyx = 10.0 * torch.log10(1.0 ** 2 / msenoisyx) #B\n",
    "            psnrxxhat = 10.0 * torch.log10(1.0 ** 2 / msexhatx) #B\n",
    "            \n",
    "            \n",
    "            ssim_psnr_values[\"ssim_noisy_x\"].append(ssimnoisyx)\n",
    "            ssim_psnr_values[\"ssim_xhat_x\"].append(ssimxxhat)\n",
    "            ssim_psnr_values[\"psnr_noisy_x\"].append(psnrnoisyx)\n",
    "            ssim_psnr_values[\"psnr_xhat_x\"].append(psnrxxhat)\n",
    "            \n",
    "            for i in range(params[\"batch_size\"]):\n",
    "                #plot images\n",
    "                axs[0,i].imshow(np.transpose(x[i].to(\"cpu\"), (1, 2, 0)), aspect='auto', origin=\"lower\") #GT\n",
    "                axs[b*2+1,i].imshow(np.transpose(noisy_x[i].to(\"cpu\"), (1, 2, 0)),aspect='auto', origin=\"lower\") #noisy\n",
    "                axs[b*2+2,i].imshow(np.transpose(x_hat[i].to(\"cpu\"), (1, 2, 0)),aspect='auto', origin=\"lower\")\n",
    "                \n",
    "\n",
    "                axs[0,i].set_xticklabels([])\n",
    "                axs[0,i].set_yticklabels([])\n",
    "                axs[b*2+1,i].set_xticklabels([])\n",
    "                axs[b*2+1,i].set_yticklabels([])\n",
    "                axs[b*2+2,i].set_xticklabels([])\n",
    "                axs[b*2+2,i].set_yticklabels([])\n",
    "                axs[0,i].set_aspect('equal')\n",
    "                axs[b*2+1,i].set_aspect('equal')\n",
    "                axs[b*2+2,i].set_aspect('equal')\n",
    "                \n",
    "                axs[8,i].set_xlabel(added_noise_labels[i], fontweight = \"bold\", fontsize = 15)\n",
    "                if i == 0:\n",
    "                    axs[0,i].set_ylabel(\"GT\", fontweight = \"bold\", fontsize = 15, rotation = 0, labelpad=20)\n",
    "                    axs[b*2+1,i].set_ylabel(f\"{b_list[b]}\", fontweight = \"bold\", fontsize = 15, rotation = 0, labelpad=20)\n",
    "                    axs[b*2+2,i].set_ylabel(f\"SDAE {b_list[b]}\", fontweight = \"bold\", fontsize = 15, rotation = 0, labelpad=40)    \n",
    "                # add ssim and psnr of each\n",
    "                \n",
    "                #axs[i,b*2+1].text(left, top, f'{ssimnoisyx[i]}\\n{psnrnoisyx[i]}',horizontalalignment='center',verticalalignment='center',rotation=45,transform=axs[i,b*2+1].transAxes)                    \n",
    "                #axs[i,b*2+2].text(left, top, f'{ssimxxhat[i]}\\n{psnrxxhat[i]}',horizontalalignment='center',verticalalignment='center',rotation=45,transform=axs[i,b*2+2].transAxes)\n",
    "                axs[b*2+1,i].text(x=220,y=1,s=f'{round(ssimnoisyx[i].item(),2)}\\n{round(psnrnoisyx[i].item(),2)}',horizontalalignment='right',verticalalignment='bottom', fontsize=10, fontweight=\"demi\", bbox=dict(facecolor='white', alpha=0.5))                    \n",
    "                axs[b*2+2,i].text(x=220,y=1,s=f'{round(ssimxxhat[i].item(),2)}\\n{round(psnrxxhat[i].item(),2)}',horizontalalignment='right',verticalalignment='bottom', fontsize=10, fontweight=\"demi\", bbox=dict(facecolor=\"white\", alpha=0.5))\n",
    "                #axs[i,0].set_axis_off()                  \n",
    "                #axs[i,b*2+1].set_axis_off()        \n",
    "                #axs[i,b*2+2].set_axis_off()        \n",
    "          \n",
    "\n",
    "    fig.suptitle('De-noising Test Images With Different Levels of Added Noise', fontsize=25, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1, top=0.95, bottom=0.05)\n",
    "    #fig.tight_layout() # made adjust not work\n",
    "    name = f\"exp1_benchmark/VISUALISE_ISIC_0.8_NOISERANGE{b_list}_REPORT.png\".replace(',', '_').replace(\"]\",\"\").replace(\"[\",\"_\").replace(\" \",\"\")\n",
    "    plt.savefig(name) \n",
    "    plt.show()\n",
    "    return ssim_psnr_values\n",
    "\n",
    "\n",
    "\n",
    "# save the psnr ssim table\n",
    "def save_images_added_report(model_b):\n",
    "    \n",
    "    shared_params = {\n",
    "     'num_epochs': 80,\n",
    "     'batch_size': 7,\n",
    "     'random_seed': 1996,\n",
    "     #'optimizer_name': 'adamW',\n",
    "     #'learning_rate': 0.001, #Alfia: 0.00001\n",
    "     'image_size': (240,240),\n",
    "     #'degrees': (-10, 10),\n",
    "     #'translate': (0.0, 0.5),\n",
    "     #'scale': (0.5, 0.95),\n",
    "     #'dropout_p': 0.5,# dropout probability\n",
    "     #'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     #'alpha':1, #weight of ssim\n",
    "     #'weight_decay':1e-3, # L2 regularization\n",
    "     'p':1\n",
    "    }\n",
    "    \n",
    "    model_file = f\"HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}epoch_80\"\n",
    "\n",
    "    b_list = [0.2,0.6,1.4,1.8]\n",
    "    #print(\"=======================================\")\n",
    "    #print(\"model_file\", model_file, \"model_b\", model_b, \"Noise_Level_Range\", b_list)\n",
    "    ssim_psnr_values = visualise_images_added_noise_report(model_file, b_list, shared_params)\n",
    "\n",
    "    #with open(f'exp1_benchmark/MODEL{model_b}_SSIM_PSNR_TABLE.pickle','wb') as handle: \n",
    "    #    pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return ssim_psnr_values\n",
    "ssim_psnr_values = save_images_added_report(0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca090fb0",
   "metadata": {},
   "source": [
    "## Added_ noise_ appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b68f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualise_images_added_noise_appendix(model_file, b_list, shared_params):\n",
    "    ssim_psnr_values = {\"ssim_noisy_x\":[],\"ssim_xhat_x\":[],\"psnr_noisy_x\":[],\"psnr_xhat_x\":[]} # b_list x B x 1 each\n",
    "    fig, axs = plt.subplots(len(b_list)*2+1,shared_params[\"batch_size\"] ,figsize=(50,50), subplot_kw={'xticks': [], 'yticks': []}) #hight same as batch_size, width = GT + len(b_list) *2\n",
    "    # build a rectangle in axes coords\n",
    "    left, width = .25, .5\n",
    "    bottom, height = .25, .5\n",
    "    right = left + width\n",
    "    top = bottom + height\n",
    "    \n",
    "\n",
    "    added_noise_labels_appendix = [\"BCC1\", \"BCC2\", \"BCC3\", \"BCC4\", \"Melanoma1\", \"Melanoma2\", \"Melanoma3\", \"Melanoma4\", \"Melanoma5\", \"Melanoma6\", \"Nevus1\", \"PBK1\", \"PBK2\"]\n",
    "    model = torch.load(model_file)\n",
    "    model.cuda()\n",
    "    \n",
    "    for b in range(len(b_list)):\n",
    "        params = { \n",
    "            \"gamma\": 0.01,\n",
    "            \"brightness\": round(b_list[b],1) # is noise level\n",
    "            }\n",
    "        params = {**shared_params, **params}\n",
    "        experiment_modules = set_experiment(model, params)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            x, noisy_x, y = next(iter(experiment_modules[\"added_noise_appendix_loader\"]))\n",
    "            \n",
    "            x, noisy_x, y = x.cuda(), noisy_x.cuda(), y.cuda()\n",
    "            outputs = model(x) \n",
    "            #y_hat = outputs['y_hat'].cuda()\n",
    "            x_hat = outputs['x_hat']\n",
    "            # qualitative scores\n",
    "            ssimnoisyxtmp = ssim(x,noisy_x, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimnoisyx = ssimnoisyxtmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            ssimxxhattmp = ssim(x,x_hat, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimxxhat = ssimxxhattmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            \n",
    "            msenoisyxtmp = mse(x,noisy_x, reduction = \"none\") # BxCxHxW\n",
    "            msexhatxtmp = mse(x,x_hat, reduction = \"none\")# BxCxHxW\n",
    "            msenoisyx = msenoisyxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            msexhatx = msexhatxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            psnrnoisyx = 10.0 * torch.log10(1.0 ** 2 / msenoisyx) #B\n",
    "            psnrxxhat = 10.0 * torch.log10(1.0 ** 2 / msexhatx) #B\n",
    "            \n",
    "            \n",
    "            ssim_psnr_values[\"ssim_noisy_x\"].append(ssimnoisyx)\n",
    "            ssim_psnr_values[\"ssim_xhat_x\"].append(ssimxxhat)\n",
    "            ssim_psnr_values[\"psnr_noisy_x\"].append(psnrnoisyx)\n",
    "            ssim_psnr_values[\"psnr_xhat_x\"].append(psnrxxhat)\n",
    "            \n",
    "            for i in range(params[\"batch_size\"]):\n",
    "                #plot images\n",
    "                axs[0,i].imshow(np.transpose(x[i].to(\"cpu\"), (1, 2, 0))) #GT\n",
    "                axs[b*2+1,i].imshow(np.transpose(noisy_x[i].to(\"cpu\"), (1, 2, 0))) #noisy\n",
    "                axs[b*2+2,i].imshow(np.transpose(x_hat[i].to(\"cpu\"), (1, 2, 0)))\n",
    "                axs[0,i].set_xticklabels([])\n",
    "                axs[0,i].set_yticklabels([])\n",
    "                axs[b*2+1,i].set_xticklabels([])\n",
    "                axs[b*2+1,i].set_yticklabels([])\n",
    "                axs[b*2+2,i].set_xticklabels([])\n",
    "                axs[b*2+2,i].set_yticklabels([])\n",
    "                axs[0,i].set_aspect('equal')\n",
    "                axs[b*2+1,i].set_aspect('equal')\n",
    "                axs[b*2+2,i].set_aspect('equal')\n",
    "                \n",
    "                axs[16,i].set_xlabel(added_noise_labels_appendix[i], fontweight = \"bold\", fontsize = 15)\n",
    "                if i == 0:\n",
    "                    axs[0,i].set_ylabel(\"GT\", fontweight = \"bold\", fontsize = 15, rotation = 0, labelpad=20)\n",
    "                    axs[b*2+1,i].set_ylabel(f\"{b_list[b]}\", fontweight = \"bold\", fontsize = 15, rotation = 0, labelpad=20)\n",
    "                    axs[b*2+2,i].set_ylabel(f\"SDAE {b_list[b]}\", fontweight = \"bold\", fontsize = 15, rotation = 0, labelpad=40)    \n",
    "                # add ssim and psnr of each\n",
    "\n",
    "                axs[b*2+1,i].text(x=220,y=1,s=f'{round(ssimnoisyx[i].item(),2)}\\n{round(psnrnoisyx[i].item(),2)}',horizontalalignment='right',verticalalignment='bottom', fontsize=10, fontweight=\"demi\", bbox=dict(facecolor='white', alpha=0.5))                    \n",
    "                axs[b*2+2,i].text(x=220,y=1,s=f'{round(ssimxxhat[i].item(),2)}\\n{round(psnrxxhat[i].item(),2)}',horizontalalignment='right',verticalalignment='bottom', fontsize=10, fontweight=\"demi\", bbox=dict(facecolor=\"white\", alpha=0.5))\n",
    "                     \n",
    "          \n",
    "\n",
    "    fig.suptitle('De-noising Test Images With Different Levels of Added Noise', fontsize=50, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    name = f\"exp1_benchmark/VISUALISE_ISIC_0.8_NOISERANGE{b_list}_APPENDIX.png\".replace(',', '_').replace(\"]\",\"\").replace(\"[\",\"_\").replace(\" \",\"\")\n",
    "    plt.savefig(name) \n",
    "    plt.show()\n",
    "    return ssim_psnr_values\n",
    "\n",
    "\n",
    "# save the psnr ssim table\n",
    "def save_images_added_appendix(model_b):\n",
    "    \n",
    "    shared_params = {\n",
    "     'num_epochs': 80,\n",
    "     'batch_size': 13,\n",
    "     'random_seed': 1996,\n",
    "     'optimizer_name': 'adamW',\n",
    "     'learning_rate': 0.001, \n",
    "     'image_size': (240,240),\n",
    "     'degrees': (-10, 10),\n",
    "     'translate': (0.0, 0.5),\n",
    "     'scale': (0.5, 0.95),\n",
    "     'dropout_p': 0.5,# dropout probability\n",
    "     'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     'alpha':1, #weight of ssim\n",
    "     'weight_decay':1e-3, # L2 regularization\n",
    "     'p':1\n",
    "    }\n",
    "    \n",
    "    model_file = f\"../HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}epoch_80\"\n",
    "\n",
    "    b_list = [0.2,0.4,0.6,0.8,1.2,1.4,1.6,1.8]\n",
    "    #print(\"=======================================\")\n",
    "    #print(\"model_file\", model_file, \"model_b\", model_b, \"Noise_Level_Range\", b_list)\n",
    "    ssim_psnr_values = visualise_images_added_noise_appendix(model_file, b_list, shared_params)\n",
    "\n",
    "    #with open(f'exp1_benchmark/MODEL{model_b}_SSIM_PSNR_TABLE.pickle','wb') as handle: \n",
    "    #    pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return ssim_psnr_values\n",
    "ssim_psnr_values = save_images_added_appendix(0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df417d40",
   "metadata": {},
   "source": [
    "# Appendix Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a6bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_images_added_noise_appendix(model_file, b_list, shared_params):\n",
    "    ssim_psnr_values = {\"ssim_noisy_x\":[],\"ssim_xhat_x\":[],\"psnr_noisy_x\":[],\"psnr_xhat_x\":[]} # b_list x B x 1 each\n",
    "    fig, axs = plt.subplots(2,3 ,figsize=(50,40), subplot_kw={'xticks': [], 'yticks': []}) #hight same as batch_size, width = GT + len(b_list) *2\n",
    "    # build a rectangle in axes coords\n",
    "    left, width = .25, .5\n",
    "    bottom, height = .25, .5\n",
    "    right = left + width\n",
    "    top = bottom + height\n",
    "\n",
    "    model = torch.load(model_file)\n",
    "    model.cuda()\n",
    "    \n",
    "    for b in range(len(b_list)):\n",
    "        params = { \n",
    "            \"gamma\": 0.01,\n",
    "            \"brightness\": round(b_list[b],1) # is noise level\n",
    "            }\n",
    "        params = {**shared_params, **params}\n",
    "        experiment_modules = set_experiment(model, params)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            x, noisy_x, y = next(iter(experiment_modules[\"added_noise_appendix_loader\"]))\n",
    "            x, noisy_x, y = x.cuda(), noisy_x.cuda(), y.cuda()\n",
    "            outputs = model(x) \n",
    "            #y_hat = outputs['y_hat'].cuda()\n",
    "            x_hat = outputs['x_hat']\n",
    "            # qualitative scores\n",
    "            \n",
    "            axs[0,0].imshow(np.transpose(x[0].to(\"cpu\"), (1, 2, 0))) #GT\n",
    "            axs[b+1,0].imshow(np.transpose(x_hat[0].to(\"cpu\"), (1, 2, 0))) #denoised\n",
    "            #axs[b*2+1,i].imshow(np.transpose(noisy_x[i].to(\"cpu\"), (1, 2, 0))) #noisy\n",
    "\n",
    "            axs[0,1].imshow(np.transpose(x[2].to(\"cpu\"), (1, 2, 0))) #GT\n",
    "            axs[b+1,1].imshow(np.transpose(x_hat[2].to(\"cpu\"), (1, 2, 0))) #denoised\n",
    "\n",
    "            axs[0,2].imshow(np.transpose(x[4].to(\"cpu\"), (1, 2, 0))) #GT\n",
    "            axs[b+1,2].imshow(np.transpose(x_hat[4].to(\"cpu\"), (1, 2, 0))) #denoised\n",
    "            \n",
    "            for i in range(3):\n",
    "                axs[0,i].set_xticklabels([])\n",
    "                axs[0,i].set_yticklabels([])\n",
    "                axs[b+1,i].set_xticklabels([])\n",
    "                axs[b+1,i].set_yticklabels([])\n",
    "                \n",
    "                axs[0,i].set_aspect('equal')\n",
    "               \n",
    "                axs[b+1,i].set_aspect('equal')\n",
    "                \n",
    "                if i == 0:\n",
    "                    axs[0,i].set_ylabel(\"GT\", fontweight = \"bold\", fontsize = 80, rotation = 0, labelpad=60)\n",
    "                    axs[1,i].set_ylabel(f\"SDAE {b_list[b]}\", fontweight = \"bold\", fontsize = 80, rotation = 0, labelpad=200)\n",
    "                    \n",
    "\n",
    "    fig.suptitle('The Removal of the Datasets Artifacts', fontsize=100, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0, top = 0.95, bottom = 0.05)\n",
    "    name = f\"../VISUALISE_ISIC_artifact_.png\"\n",
    "    plt.savefig(name) \n",
    "    plt.show()\n",
    "    return ssim_psnr_values\n",
    "\n",
    "\n",
    "# save the psnr ssim table\n",
    "def save_images_added_appendix(model_b):\n",
    "    \n",
    "    shared_params = {\n",
    "     'num_epochs': 80,\n",
    "     'batch_size': 5,\n",
    "     'random_seed': 1996,\n",
    "     'optimizer_name': 'adamW',\n",
    "     'learning_rate': 0.001, \n",
    "     'image_size': (240,240),\n",
    "     'degrees': (-10, 10),\n",
    "     'translate': (0.0, 0.5),\n",
    "     'scale': (0.5, 0.95),\n",
    "     'dropout_p': 0.5,# dropout probability\n",
    "     'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     'alpha':1, #weight of ssim\n",
    "     'weight_decay':1e-3, # L2 regularization\n",
    "     'p':1\n",
    "    }\n",
    "    \n",
    "    model_file = f\"HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}epoch_80\"\n",
    "\n",
    "    b_list = [0.6]\n",
    "    ssim_psnr_values = visualise_images_added_noise_appendix(model_file, b_list, shared_params)\n",
    "    return ssim_psnr_values\n",
    "ssim_psnr_values = save_images_added_appendix(0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9737dd6",
   "metadata": {},
   "source": [
    "## Natural Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fb401",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualise_images_natural(model_file, shared_params):\n",
    "    ssim_psnr_values = {\"ssim_noisy_x\":[],\"ssim_xhat_x\":[],\"psnr_noisy_x\":[],\"psnr_xhat_x\":[]} # b_list x B x 1 each\n",
    "    fig, axs = plt.subplots(2, shared_params[\"batch_size\"] ,figsize=(25,10), subplot_kw={'xticks': [], 'yticks': []}) #hight same as batch_size, width = GT + len(b_list) *2\n",
    "    # build a rectangle in axes coords\n",
    "    left, width = .25, .5\n",
    "    bottom, height = .25, .5\n",
    "    right = left + width\n",
    "    top = bottom + height\n",
    "    model = torch.load(model_file)\n",
    "    model.cuda()\n",
    "    \n",
    "    params = { \n",
    "        \"gamma\": 0.01,\n",
    "        \"brightness\": 0.6 # is dummy value\n",
    "        }\n",
    "    params = {**shared_params, **params}\n",
    "    experiment_modules = set_experiment(model, params)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        x, noisy_x, y = next(iter(experiment_modules[\"natural_loader\"]))\n",
    "        x, noisy_x, y = x.cuda(), noisy_x.cuda(), y.cuda()\n",
    "        outputs = model(x) \n",
    "        #y_hat = outputs['y_hat'].cuda()\n",
    "        x_hat = outputs['x_hat']\n",
    "        print(y)\n",
    "        # qualitative scores\n",
    "        \n",
    "        #plot images\n",
    "        for i in range(params[\"batch_size\"]):\n",
    "            axs[0,i].imshow(np.transpose(x[i].to(\"cpu\"), (1, 2, 0))) #original image\n",
    "            axs[1,i].imshow(np.transpose(x_hat[i].to(\"cpu\"), (1, 2, 0)))\n",
    "            axs[0,i].set_xticklabels([])\n",
    "            axs[0,i].set_yticklabels([])\n",
    "            axs[1,i].set_xticklabels([])\n",
    "            axs[1,i].set_yticklabels([])\n",
    "            axs[0,i].set_aspect('equal')\n",
    "            axs[1,i].set_aspect('equal')\n",
    "            \n",
    "            axs[1,i].set_xlabel(experiment_modules[\"natural_labels\"][i], fontweight = \"bold\", fontsize = 25)\n",
    "            if i == 0:\n",
    "                axs[0,i].set_ylabel(\"Original Image\", fontweight = \"bold\", fontsize = 25)\n",
    "                axs[1,i].set_ylabel(\"Denoised Image\", fontweight = \"bold\", fontsize = 25)\n",
    "\n",
    "       \n",
    "    fig.suptitle(f'De-noising Naturally \"Bad Light\" Images', fontsize=30, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    name = f\"../VISUALISE_ISIC_0.8_NATURAL.png\"\n",
    "    plt.savefig(name) \n",
    "    plt.show()\n",
    "    return ssim_psnr_values\n",
    "\n",
    "\n",
    "# save the psnr ssim table\n",
    "def save_images_natural(model_b):\n",
    "    \n",
    "    shared_params = {\n",
    "    \n",
    "     'batch_size': 5,\n",
    "     'random_seed': 1996,     \n",
    "     'image_size': (240,240),     \n",
    "     'n_classes': 4,\n",
    "     'p':0\n",
    "    }\n",
    "    \n",
    "    model_file = f\"HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS{model_b}epoch_80\"\n",
    "\n",
    "    print(\"=======================================\")\n",
    "    print(\"model_file\", model_file, \"model_b\", model_b)\n",
    "    ssim_psnr_values = visualise_images_natural(model_file, shared_params)\n",
    "\n",
    "    #with open(f'exp1_benchmark/MODEL{model_b}_SSIM_PSNR_TABLE.pickle','wb') as handle: \n",
    "    #    pickle.dump(dic, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return ssim_psnr_values\n",
    "ssim_psnr_values = save_images_natural(0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e21cc2",
   "metadata": {},
   "source": [
    "# Reconstruction in different epochs fixed noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4019a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def visualise_images_epochs(epoch_list, b, shared_params):\n",
    "    ssim_psnr_values = {\"ssim_noisy_x\":[],\"ssim_xhat_x\":[],\"psnr_noisy_x\":[],\"psnr_xhat_x\":[]} # b_list x B x 1 each\n",
    "    fig, axs = plt.subplots(shared_params[\"batch_size\"], len(epoch_list)+2 ,figsize=(20,20), subplot_kw={'xticks': [], 'yticks': []}) #hight same as batch_size, width = GT + len(b_list) *2\n",
    "\n",
    "    added_noise_labels = [\"Melanoma\", \"Nevus\", \"BCC\", \"PBK\"]\n",
    "    for m in range(len(epoch_list)):\n",
    "        model_file = f\"HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8epoch_{epoch_list[m]}\"\n",
    "        model = torch.load(model_file)\n",
    "        model.cuda()\n",
    "        params = { \n",
    "            \"gamma\": 0.01,\n",
    "            \"brightness\": round(b,1) # is noise level\n",
    "            }\n",
    "        params = {**shared_params, **params}\n",
    "        experiment_modules = set_experiment(model, params)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            x, noisy_x, y = next(iter(experiment_modules[\"added_noise_loader\"]))\n",
    "            x, noisy_x, y = x.cuda(), noisy_x.cuda(), y.cuda()\n",
    "            outputs = model(noisy_x) \n",
    "            #y_hat = outputs['y_hat'].cuda()\n",
    "            x_hat = outputs['x_hat']\n",
    "            # qualitative scores\n",
    "            ssimnoisyxtmp = ssim(x,noisy_x, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimnoisyx = ssimnoisyxtmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            ssimxxhattmp = ssim(x,x_hat, 5) #same dimension as x and noisy_x BxCxHxW\n",
    "            ssimxxhat = ssimxxhattmp.mean(-1).mean(-1).mean(-1) # size = B\n",
    "            \n",
    "            msenoisyxtmp = mse(x,noisy_x, reduction = \"none\") # BxCxHxW\n",
    "            msexhatxtmp = mse(x,x_hat, reduction = \"none\")# BxCxHxW\n",
    "            msenoisyx = msenoisyxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            msexhatx = msexhatxtmp.mean(-1).mean(-1).mean(-1) #B\n",
    "            psnrnoisyx = 10.0 * torch.log10(1.0 ** 2 / msenoisyx) #B\n",
    "            psnrxxhat = 10.0 * torch.log10(1.0 ** 2 / msexhatx) #B\n",
    "            \n",
    "            \n",
    "            ssim_psnr_values[\"ssim_noisy_x\"].append(ssimnoisyx)\n",
    "            ssim_psnr_values[\"ssim_xhat_x\"].append(ssimxxhat)\n",
    "            ssim_psnr_values[\"psnr_noisy_x\"].append(psnrnoisyx)\n",
    "            ssim_psnr_values[\"psnr_xhat_x\"].append(psnrxxhat)\n",
    "            \n",
    "            for i in range(params[\"batch_size\"]):\n",
    "                #plot images\n",
    "                axs[i,0].imshow(np.transpose(x[i].to(\"cpu\"), (1, 2, 0))) #GT\n",
    "                axs[i,1].imshow(np.transpose(noisy_x[i].to(\"cpu\"), (1, 2, 0))) #noisy\n",
    "                axs[i,m+2].imshow(np.transpose(x_hat[i].to(\"cpu\"), (1, 2, 0)))\n",
    "                axs[i,0].set_xticklabels([])\n",
    "                axs[i,0].set_yticklabels([])\n",
    "                axs[i,1].set_xticklabels([])\n",
    "                axs[i,1].set_yticklabels([])\n",
    "                axs[i,m+2].set_xticklabels([])\n",
    "                axs[i,m+2].set_yticklabels([])\n",
    "                axs[i,0].set_aspect('equal')\n",
    "                axs[i,1].set_aspect('equal')\n",
    "                axs[i,m+2].set_aspect('equal')\n",
    "                \n",
    "                if i == params[\"batch_size\"]+1:\n",
    "                    axs[i,0].set_xlabel(\"GT\", fontweight = \"bold\", fontsize = 25, rotation=45)\n",
    "                    axs[i,1].set_xlabel(f\"Noisy({b})\", fontweight = \"bold\", fontsize = 25, rotation=45)\n",
    "                    axs[i,m+2].set_xlabel(f\"Epoch {epoch_list[m]}\", fontweight = \"bold\", fontsize = 25, rotation=45)\n",
    "                \n",
    "                if m == 0: \n",
    "                    axs[i,m].set_ylabel(added_noise_labels[i], fontweight = \"bold\", fontsize = 25, rotation=90)\n",
    "                # add ssim and psnr of each\n",
    "\n",
    "                axs[i,m+2].text(x=220,y=220,s=f'{round(ssimxxhat[i].item(),2)}\\n{round(psnrxxhat[i].item(),2)}',horizontalalignment='right',verticalalignment='bottom', fontsize=25, fontweight=\"demi\", bbox=dict(facecolor='white', alpha=0.5))\n",
    "                axs[i,1].text(x=220,y=220,s=f'{round(ssimnoisyx[i].item(),2)}\\n{round(psnrnoisyx[i].item(),2)}',horizontalalignment='right',verticalalignment='bottom', fontsize=25, fontweight=\"demi\", bbox=dict(facecolor='white', alpha=0.5))\n",
    "                \n",
    "\n",
    "    fig.suptitle('Reconstruction at Fixed Noise Level in Different Epochs', fontsize=35, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "   \n",
    "    name = f\"exp1_benchmark/VISUALISE_ISIC_0.8_LARGEEPOCH_b{b}{epoch_list}.png\".replace(',', '_').replace(\"]\",\"\").replace(\"[\",\"_\").replace(\" \",\"\")\n",
    "    plt.savefig(name)     \n",
    "    plt.show()\n",
    "    return ssim_psnr_values\n",
    "\n",
    "# save the psnr ssim table for different epochs\n",
    "def save_images_epochs(b, epoch_list):\n",
    "    \n",
    "    shared_params = {\n",
    "    \n",
    "     'batch_size': 4,\n",
    "     'random_seed': 1996,\n",
    "     'optimizer_name': 'adamW',\n",
    "     'learning_rate': 0.001, \n",
    "     'image_size': (224,224),\n",
    "     'degrees': (-10, 10),\n",
    "     'translate': (0.0, 0.5),\n",
    "     'scale': (0.5, 0.95),\n",
    "     'dropout_p': 0.5,# dropout probability\n",
    "     'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     'alpha':1, #weight of ssim\n",
    "     'weight_decay':1e-3, # L2 regularization\n",
    "     'p':1\n",
    "    }\n",
    "    \n",
    "    print(\"=======================================\")\n",
    "    print(\"Noise_Level\", b, \"epoch range\", epoch_list)\n",
    "    ssim_psnr_values = visualise_images_epochs(epoch_list, b, shared_params)\n",
    "    return ssim_psnr_values\n",
    "\n",
    "b = 0.6\n",
    "epoch_list = [10,40,80]\n",
    "ssim_values = save_images_epochs(b, epoch_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9218e73",
   "metadata": {},
   "source": [
    "# Loading the tables to use in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(ssim_psnr_table_path):\n",
    "    with open(ssim_psnr_table_path, 'rb') as f:\n",
    "        x = pickle.load(f)\n",
    "\n",
    "    #data_items = x.items()\n",
    "    #data_list = list(data_items)\n",
    "    #df = pd.DataFrame(data_list)\n",
    "    df = pd.DataFrame.from_dict(x)\n",
    "\n",
    "\n",
    "    #ssim with 3 decimal\n",
    "    df[\"mean_ssim_noisy_x\"] = df[\"mean_ssim_noisy_x\"].round(2)\n",
    "    df[\"mean_ssim_xhat_x\"] = df[\"mean_ssim_xhat_x\"].round(2)\n",
    "    # psnr with 2 decimal\n",
    "    df[\"mean_psnr_noisy_x\"] = df[\"mean_psnr_noisy_x\"].round(2)\n",
    "    df[\"mean_psnr_xhat_x\"] = df[\"mean_psnr_xhat_x\"].round(2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with 0.8\n",
    "ssim_psnr_table_path = \"../MODEL0.8_SSIM_PSNR_TABLE_train.pickle\"\n",
    "df = get_table(ssim_psnr_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb465c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2bdbad",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169698cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch-12a48cd1e573\n",
    "# visualising feature map of an image: noisy and not noisy\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def layer_visualise(model, experiment_modules):\n",
    "    names = [\"image\", \"conv1\", \"conv2\", \"conv3\", \"conv4\", \"deconv4\", \"deconv3\", \"deconv2\", \"deconv1\"]\n",
    "    #for x, noisy_x, y in experiment_modules[\"added_noise_loader\"]:\n",
    "    x, noisy_x, y = next(iter(experiment_modules[\"added_noise_loader\"]))\n",
    "    outputs = []\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        outputs.append(x)\n",
    "\n",
    "        image1 = model.conv1(x)\n",
    "        image1 = model.act_fn(image1)\n",
    "        \n",
    "        outputs.append(image1)\n",
    "        image2 = model.conv2(image1)\n",
    "        image2 = model.act_fn(image2)\n",
    "        outputs.append(image2)\n",
    "        image3 = model.conv3(image2)\n",
    "        image3 = model.act_fn(image3)\n",
    "        outputs.append(image3)\n",
    "        image4 = model.conv4(image3)\n",
    "        image4 = model.act_fn(image4)\n",
    "        outputs.append(image4)\n",
    "\n",
    "        image5 = model.deconv4(image4)\n",
    "        image5 = model.act_fn(image5)\n",
    "        outputs.append(image5)\n",
    "\n",
    "        image6 = model.deconv3(torch.cat((image5,image5),1))\n",
    "        image6 = model.act_fn(image6)\n",
    "        outputs.append(image6)\n",
    "\n",
    "        image7 = model.deconv2(torch.cat((image6,image6),1))\n",
    "        image7 = model.act_fn(image7)\n",
    "        outputs.append(image7)\n",
    "\n",
    "        image8 = model.deconv1(torch.cat((image7,image7),1))\n",
    "        image8 = model.out_fn(image8)\n",
    "        outputs.append(image8)\n",
    "\n",
    "    #from 3d to 2D\n",
    "\n",
    "    processed = []\n",
    "    for feature_map in outputs:\n",
    "        feature_map = feature_map.squeeze(0)\n",
    "        \n",
    "        gray_scale = torch.sum(feature_map,0)\n",
    "        gray_scale = gray_scale / feature_map.shape[0]\n",
    "        processed.append(gray_scale.data.cpu().numpy())\n",
    "    fig, axs = plt.subplots(2,5,figsize=(30,15))\n",
    "    fig.delaxes(axs[1,4])\n",
    "    for i in range(5):\n",
    "        im = axs[0,i].imshow(processed[i], cmap='gray')\n",
    "        divider = make_axes_locatable(axs[0,i])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "        axs[0,i].axis(\"off\")\n",
    "        axs[0,i].set_title(names[i].split('(')[0], fontsize=25)\n",
    "    for i in range(4):\n",
    "        \n",
    "        im = axs[1,i].imshow(processed[i+5], cmap='gray')\n",
    "        divider = make_axes_locatable(axs[1,i])\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "        axs[1,i].axis(\"off\")\n",
    "        axs[1,i].set_title(names[i+5].split('(')[0], fontsize=25)\n",
    "            \n",
    "    fig.suptitle('Feature Maps of a Sample', fontsize=35, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1, top=0.95, bottom=0.05)\n",
    "    name = f\"../MODEL_ISIC.png\"\n",
    "    plt.savefig(name) \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return\n",
    "\n",
    "def get_layer_visualisation():\n",
    "    model_file = \"HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8epoch_80\"\n",
    "\n",
    "    shared_params = {\n",
    "     'num_epochs': 80,\n",
    "     'batch_size': 1,\n",
    "     'random_seed': 1996,\n",
    "     'optimizer_name': 'adamW',\n",
    "     'learning_rate': 0.001, #Alfia: 0.00001\n",
    "     'image_size': (240,240),\n",
    "     'degrees': (-10, 10),\n",
    "     'translate': (0.0, 0.5),\n",
    "     'scale': (0.5, 0.95),\n",
    "     'dropout_p': 0.5,# dropout probability\n",
    "     'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     'alpha':1, #weight of ssim\n",
    "     'weight_decay':1e-3 # L2 regularization\n",
    "    }\n",
    "    params = { \n",
    "\n",
    "            \"gamma\": 0.01,\n",
    "            \"p\": 1,\n",
    "            \"brightness\": 0.6 # from 0.8 til 1.2\n",
    "        }\n",
    "    params = {**shared_params, **params} \n",
    "    model = torch.load(model_file)\n",
    "    model.to(\"cpu\")\n",
    "    experiment_modules = set_experiment(model, params)\n",
    "    layer_visualise(model, experiment_modules)\n",
    "    return \n",
    "\n",
    "get_layer_visualisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7787950",
   "metadata": {},
   "source": [
    "# Visualising filters of conv1 and deconv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb650c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_filters_single_channel(t):\n",
    "    #https://github.com/Niranjankumar-c/DeepLearning-PadhAI/blob/master/DeepLearning_Materials/6_VisualizationCNN_Pytorch/CNNVisualisation.ipynb\n",
    "    #kernels depth * number of kernels\n",
    "    nplots = t.shape[0]*t.shape[1]\n",
    "    ncols = 12\n",
    "    \n",
    "    nrows = 1 + nplots//ncols\n",
    "    #convert tensor to numpy image\n",
    "    npimg = np.array(t.numpy(), np.float32)\n",
    "    \n",
    "    count = 0\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    print(t.shape[0],t.shape[1])\n",
    "    #looping through all the kernels in each channel\n",
    "    for i in range(t.shape[0]):\n",
    "        for j in range(t.shape[1]):\n",
    "            count += 1\n",
    "            ax1 = fig.add_subplot(nrows, ncols, count)\n",
    "            npimg = np.array(t[i, j].numpy(), np.float32)\n",
    "            npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
    "            npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
    "            if j == 0:\n",
    "                ax1.imshow(npimg, cmap='Reds')\n",
    "            if j == 1:\n",
    "                ax1.imshow(npimg, cmap='Greens')\n",
    "            if j == 2:\n",
    "                ax1.imshow(npimg, cmap='Blues')\n",
    "            #ax1.set_title(str(i) + ',' + str(j))\n",
    "            ax1.axis('off')\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "    fig.suptitle('Filters of the First Convolution', fontsize=25, fontweight=\"bold\")\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1, top = 0.95, bottom = 0.05)\n",
    "    #fig.tight_layout() # made adjust not work\n",
    "    name = f\"exp1_benchmark/con_kernel_ISIC.png\"\n",
    "    plt.savefig(name) \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_filters():\n",
    "    model_file = \"HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8/HPO_MAIN_BENCHMARK_CORRECTED_GAMMA0.01_P0.6_BRIGHTNESS0.8epoch_80\"\n",
    "\n",
    "    shared_params = {\n",
    "     'num_epochs': 80,\n",
    "     'batch_size': 1,\n",
    "     'random_seed': 1996,\n",
    "     'optimizer_name': 'adamW',\n",
    "     'learning_rate': 0.001,\n",
    "     'image_size': (240,240),\n",
    "     'degrees': (-10, 10),\n",
    "     'translate': (0.0, 0.5),\n",
    "     'scale': (0.5, 0.95),\n",
    "     'dropout_p': 0.5,# dropout probability\n",
    "     'negative_slope': 0.2, # negative slope for LeakyRelu\n",
    "     'n_classes': 4,\n",
    "     'alpha':1, #weight of ssim\n",
    "     'weight_decay':1e-3 # L2 regularization\n",
    "    }\n",
    "    params = { \n",
    "\n",
    "            \"gamma\": 0.01,\n",
    "            \"p\": 1,\n",
    "            \"brightness\": 1.4\n",
    "        }\n",
    "    params = {**shared_params, **params} \n",
    "    model = torch.load(model_file)\n",
    "    model.to(\"cpu\")\n",
    "    experiment_modules = set_experiment(model, params)\n",
    "    weight_tensor = model.conv1.weight.data \n",
    "    plot_filters_single_channel(weight_tensor.cpu())\n",
    "\n",
    "    return \n",
    "get_filters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d374391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
